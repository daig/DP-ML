\documentclass{beamer}

\newcommand{\epsdelt}{$(\epsilon,\delta)-$}
\usetheme{Warsaw}

\title{Differential Privacy}

% A subtitle is optional and this may be deleted
\subtitle{for Machine Learning}

\author{Dai}


\date{February 11, 2016}




% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

% Section and subsections will appear in the presentation overview
% and table of contents.

\section{Introducing Differential Privacy}
\begin{frame}{What is Privacy?}
  \begin{itemize}
    \item ``Publishing Data Won't harm you''
    \item Two ways to give privacy: \begin{itemize}
      \item Erasure
      \item Random Noise (Plausible Deniability)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Erasure Example: Netflix Prize}
\begin{itemize}
  \item Netflix Challenge provides anonymized user profiles for analytics
  \begin{itemize}
    \item Profiles include Movie ratings, review dates
    \item 1mil+ ratings, ~480k users, ~18k movies
  \end{itemize}
  \item 96\% of released profiles can be identified with at most 8 rating dates by matching to IMDb
\end{itemize}
\end{frame}

\begin{frame}{Randomization Example: Randomized Responses}
  \begin{itemize}
    \item For collecting sensitive data: \begin{enumerate}
      \item Flip a coin
      \item If Tails, record truthfully
      \item If Heads, flip again and record ``yes'' if Heads, ``no'' if Tails
    \end{enumerate}
    \item Provides ``Plausible Deniability''
  \end{itemize}
\end{frame}

\begin{frame}{The impossibility of Perfect Privacy}
  \begin{itemize}
    \item Linkage Attacks: Correlated datasets link erased values
    \item Re-sampling: Correlated queries average noisy values
    \item Both methods provide {\it some} privacy, but how much? (Not~enough)
  \end{itemize}
  \begin{definition}[Differential Privacy, Informally]
    ``Opting into a Differentially Private database won't harm you''
  \end{definition}
\end{frame}

\begin{frame}{The Promise of Differential Privacy}
  \begin{itemize}
    \item[\checkmark] Limit the total {\it information}, of any kind, provided by a query
    \item[\checkmark] Conceptually: Small changes to the database lead to small changes in expected output
    \item[$\times$]NOT guaranteed: Public Linkage
  \end{itemize}
  \begin{example}[Public Linkage]
  Alice smokes (publicly).
  A study is published linking smoking to cancer.
  Alice is not in the study, but her insurance goes up.
  \end{example}
  \pause
  \begin{block}{}
    \alert{Participating in a study can be {\it no worse} than not participating}
  \end{block}
\end{frame}

\begin{frame}{Differential Privacy}
    Given: Noisy query $Q$, Information $\epsilon$, Probability $\delta$
  \begin{definition}[$Q$ is $(\epsilon,\delta)-$Differentially Private]
    $\forall$ Neighbor Datasets: $||x-x'||_1 \leq 1$\\
    $\forall$ Selection Criteria: $S \subseteq Range(Q)$
    \[P(Q(x)\in S) \leq e^{\epsilon} P(Q(x') \in S) + \delta\]
    Equivalently:
    \[\left| \log{\frac{P(Q(x)=a)}{P(Q(x')=a)}}\right| \leq \epsilon \quad \mbox{with probability at least } 1 - \delta\]
  \end{definition}
\end{frame}


\begin{frame}{Redux: Randomized Responses}
  Recall:
  \begin{enumerate}
    \item Flip a coin
    \item Answer truthfully only if tails
    \item Else flip again and answer Heads=T, Tails=F
  \end{enumerate}
  {\bf Proposition:} This mechanism has Has $(\ln{3},0)-$ DP ($~1.6$ bits)
\begin{proof}
  The max difference is between
  \center $P_{T|T} = P(\mbox{Response=T} | \mbox{Actual=T}) = 3/4$
  \center $P_{T|F} = P(\mbox{Response=T} | \mbox{Actual=F}) = 1/4$\\
so\\
$log{\frac{P_{T|T}}{P_{T|F}}} = \log{\frac{3/4}{1/4}} = \log{3}$
\end{proof}
\end{frame}

\begin{frame}{General Masking: Laplace Noise}
  For deterministic $q$, add Laplace Noise independently to each component
  \begin{definition}[L1-sensitivity]
    \[\Delta_q = \max_{x,x'} ||q(x) - q(x')||_1\]
  \end{definition}
  \begin{definition}[Laplace / ``Symmetric Exponential'' Distribution]
  \[  L^x_{\Delta q/\epsilon}(z) = \frac{\epsilon}{2\Delta_q} e^{\frac{-\epsilon}{\Delta_q}|q(x)-z|}\]
  \end{definition}
\end{frame}

\begin{frame}{Not Too Wrong}
  \begin{block}{For query $q$ returning a $k-$vector, and $1 \geq d > 0$}
    \[P\left(\left|\left|q(x)-L^x_{\Delta q/\epsilon}\right|\right|_{\infty} \geq \log{\left(\frac{k}{d}\right)}\frac{\Delta_q}{\epsilon}\right) \leq d\]
  \end{block}
  \begin{example}{Count $10000$ interesting google searches}
    \begin{itemize}
      \item Each person can affect only one bin, so $\Delta_f=1$
      \item For $\epsilon=1$, with $d=95\%$ probability, no count will be off by more than $\ln{\frac{10000}{0.05} \approx 12.2}$
      \item[\checkmark] Independent of total population size!
      \item[$\times$] But bad for outlier analysis...
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}{Useful $(\epsilon,0)-$DP Algorithms}
  \begin{itemize}
    \item Folds: Max, Count, Mean, etc.
    \item $K-$means
    \item $K-$medians
    \item Vertex Cover
    \item Empirical risk minimization
    \item More discovered frequently!
  \end{itemize}
\end{frame}

\section{Compositions}
\begin{frame}{Compositions}
  \begin{itemize}
    \item Proving DP for an algorithm is tedious
    \pause
    \item Fortunately, DP algorithms compose cleanly!
    \item Nice theoretical structure guides good implementation
  \end{itemize}
\end{frame}
\begin{frame}{Query Chaining}
\begin{block}{Post-Processing (Functor Instance)}
  If $Q$ is \epsdelt DP then\\
  $\forall f : Range(Q) \to Range(Q)$, $f \circ Q$ is also \epsdelt DP
\end{block}

\begin{proof}[Post-Processing Immunity]
  \[\log{\frac{P(g(Q(x)) \in S) - \delta}{P(g(Q(x')) \in S)}} =
  \log{\frac{P(Q(x) \in g^{-1}(S)) - \delta}{P(Q(x') \in g^{-1}(S))}}
  \leq \epsilon \]
\end{proof}
\begin{block}{Successive queries are additive (Applicative Instance)}
If $Q$ is \epsdelt DP and $Q'$ is $(\epsilon',\delta')-$DP\\
then $Q \circ Q'$ is $(\epsilon' + \epsilon, \delta' + \delta)-$ DP
\end{block}
\end{frame}

\begin{frame}{Pre-Processing}
  \begin{definition}[Group Privacy]
    For databases differing in more entries at once:
    If $Q$ is \epsdelt DP then:\\
    For groups $||x-x'||_1 \leq k$, $Q$ is $(k\epsilon,ke^{(k-1)\epsilon}\delta)-${\it Group Private}
  \end{definition}
  \begin{definition}[$k-$continuous]
    $f: A \to B$ is $k-$continuous if $||f(x)-f(x')|| \leq k||x-x'||$
  \end{definition}
\begin{block}{Pre-Processing (Contravariant Instance)}
  In particular, if $Q$ is \epsdelt DP, and $f$ is $k-$continuous, then
  \[Q \circ f \mbox{ is } (k\epsilon,ke^{(k-1)\epsilon}\delta)-\mbox{DP}\]
\end{block}
\pause
\begin{block}{}
  \center \alert{Only feasible for $(\epsilon,0)-$DP queries!}
\end{block}
\end{frame}




\begin{frame}{Adaptive Chaining (Monad Instance)}
  \begin{itemize}
    \item In reality, adversaries can choose queries based on past results
    \item Instead, ask what queries they {\it would make}, if the answer turned out a certain way \begin{itemize}
      \item i.e. ``Continuation Passing'' form
      \item Sew the queries together at the server end by correlating the noise
    \end{itemize}
  \end{itemize}
  \begin{block}{Naive adaptive composition of $k$ \epsdelt DP queries}
    \[(\sqrt{2k\ln{(1/\delta')}} \epsilon + k \epsilon(e^{\epsilon}-1),k\delta + \delta')\mbox{-DP}\]
    But clever methods allow exponential (in DB size) chaining, by ``preparing'' queries
  \end{block}
  \pause
  \begin{block}{}
    \center    \alert{Still under active research}

  \end{block}

\end{frame}

\section{Applications to Machine Learning}

\begin{frame}{The Problem of ``P-Hacking''}
  \begin{itemize}
    \item Over-fitting is a serious problem in machine learning
    \item Over-fitting is a serious \alert{Practical} problem in biology
    \pause
    \item Typical confidence statistics assume data and analysis are uncorrelated (almost never true!)
    \item Conservative analysis requires partitioning precious test data
  \end{itemize}
\end{frame}
\begin{frame}{Privacy $\approx$ Stability}
  \begin{itemize}
    \item ``Stable'' ML processes resistant to overfitting item
    \item ``Differential Privacy'' looks a lot like ``Stability'' \begin{itemize}
      \item DP limits training-specific entropy leaks
    \end{itemize}
  \end{itemize}
\begin{quote}
  ``any adaptive analysis that is carried
out in a differentially private manner must lead to a conclusion
that generalizes to the underlying distribution''
\end{quote}
\end{frame}

\begin{frame}{``Thwarting'' Adaptive Analysis}
  \begin{itemize}
    \item Many Stable ML algorithms exist, but DP gives us {\it composition}
    \item Adaptive Privacy can be reused for Adaptive Analysis
  \end{itemize}
\end{frame}
\begin{frame}{Transfer Theorem}
  \begin{definition}[Informal]
    The {\it Sample Complexity} of an analysis is the number
  of i.i.d. samples needed to ``safely'' perform the analysis
  \end{definition}
  \begin{block}{Major Result}
    For $r$ rounds of adaptive analysis, totaling $m$ queries in $X$, accurate to tolerance $t$,
    has Sample Complexity the minimum of
    \begin{itemize}
      \item $O\left(\frac{(\log{m}^{3/2})\sqrt{\log{|X|}}}{t^{7/2}}\right)$ \hfill (slow)
      \item $O\left(\frac{\log{m}\log{|X|}}{t^4}\right)$\hfill (slow)
      \item $O\left(\frac{\sqrt{m}(\log{m})^{3/2}}{t^{5/2}}\right)$ \hfill(fast)
      \item $O\left(\frac{r\log{m}}{t^2}\right)$\hfill (fast)
    \end{itemize}
  \end{block}
\end{frame}


% All of the following is optional and typically not needed.
\appendix
\section<presentation>*{\appendixname}
\subsection<presentation>*{For Further Reading}

\begin{frame}
  \frametitle<presentation>{For Further Reading}

  \begin{thebibliography}{10}

  \beamertemplatebookbibitems
  % Start with overview books.

  \bibitem{PrivBook}
    C.~Dwork, A.~Roth
    \newblock {\em The Algorithmic Foundations of Differential Privacy}.
    \newblock Foundations and Trends in Theoretical Computer Science, 2014.


  \beamertemplatearticlebibitems
  % Followed by interesting articles. Keep the list short.

  \bibitem{Preserving}
    C.~Dwork et. al.
    \newblock Preserving Statistical Validity in Adaptive Data Analysis.
    \newblock STOC, 2015.

  \bibitem{Linear}
  M.~Gaboardi, A.~Haeberlen, J.~Hsu, A.~Narayan, B.~Pierce
  \newblock Linear Dependent Types for Differential Privacy.
  \newblock POPL, 2013
  \end{thebibliography}
\end{frame}

\begin{frame}{APPENDIX}

\end{frame}

\begin{frame}{Accounting For Utility}
  \begin{itemize}
    \item Consider an auction: \begin{itemize}
      \item Bidders confidentially provide their maximum bid
      \item The final sale price is made public
      \item Can't just add any noise, or it won't sell
    \end{itemize}
    \item More generally, a map $u(DB,QueryResult) \to Utility$ \begin{itemize}
      \item When $u(x,z) = ||q(x)-z||$, recover the usual case.
    \end{itemize}
    \[\ln{\frac{\mbox{exp}(\epsilon u(x,r)/\Delta_u)}{\mbox{exp}(\epsilon u(y,r)/\Delta_u}} = \epsilon [u(x,r)-u(y,r)]/\Delta_u \leq \epsilon\]
  \end{itemize}
\end{frame}

\end{document}
